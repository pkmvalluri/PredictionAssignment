---
title: "Practical Machine Learning - Prediction Assignment Markdown document"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Prediction Assignment 

The goal of this project is to predict the manner in which they did the exercise and to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. 

This is the "classe" variable in the training set.Also model to predict 20 different test cases.

## Include caret and load files

First, I loaded the data both from the provided training and test data provided by COURSERA. Some values contained a "#DIV/0!" that I replaced with an NA value: The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.


```{r}
library(caret)
setwd("C:\\Krishna\\CourseRa\\Practicle Machine Learning\\Week4\\PredictionAssignment")

trainingFile <- "pml_training.csv"
url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"

download.file(url, destfile = trainingFile )

training <- read.csv(trainingFile , na.strings = c("NA","#DIV/0!",""))

testingFile <- "pml_testing.csv"

url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url, destfile = testingFile )

testing <- read.csv(testingFile, na.strings = c("NA","#DIV/0!",""))

```

## (Clean and) Remove invalid predictors

Remove columns with Near Zero Values and Remove columns with NA or is empty

```{r}
subTrain <- training[, names(training)[!(nzv(training, saveMetrics = T)[, 4])]]

subTrain <- subTrain[, names(subTrain)[sapply(subTrain, function (x) ! (any(is.na(x) | x == "")))]]
```

## Remove V1 which seems to be a serial number, and
cvtd_timestamp that is unlikely to influence the prediction

```{r}
subTrain <- subTrain[,-1]
subTrain <- subTrain[, c(1:3, 5:58)]
```

##Separate the data to be used for Cross Validation
 Divide the training data into a training set and a validation set
 
```{r}
inTrain <- createDataPartition(subTrain$classe, p = 0.6, list = FALSE)
subTraining <- subTrain[inTrain,]
subValidation <- subTrain[-inTrain,]
```

# Create the prediction model using random forest

Check if model file exists from previous run and as aadvised by the forums running on mutiple cores takes a long time so loading the model from previous runs



```{r}
model <- "modelFit.RData"
if (!file.exists(model)) {

    # If not, set up the parallel clusters.  
    require(parallel)
    require(doParallel)
    cl <- makeCluster(detectCores() - 1)
    registerDoParallel(cl)
    
    fit <- train(subTraining$classe ~ ., method = "rf", data = subTraining)
    save(fit, file = "modelFit.RData")
    
    stopCluster(cl)
} else {
    # model exists from previous run, load it and use it.  
    load(file = "modelFit.RData", verbose = TRUE)
}
```


#Measure the Accuracy and Sample Error of the prediction model

Measure its accuracy using confusion matrix
 
```{r}
predTrain <- predict(fit, subTraining)

confusionMatrix(predTrain, subTraining$classe)

```

#Using the validation subset and create a prediction. Then measure it’s accuracy. From the training subset, the accuracy is very high, at above 99%. The sample error is 0.0008.


```{r}
predValidation <- predict(fit, subValidation)
confusionMatrix(predValidation, subValidation$classe)

```



#The accuracy of this model is high and no need to build another prediction model for better accuracy to stack multiple prediction models. takes too long a time to run another training process. Given the level of accuracy I am fine with this prediction model. 

From the model, the following are the list of important predictors in the model.

```{r}
varImp(fit)

```

Based on the validation accuracy at over 99% and Cross-Validation out-of-sample error rate of 0.03%, with CI between 99.87% to 99.97%, the prediction model should be applied to the final testing set, and predict the classe in the 20 test cases.

```{r}

fit$finalModel
```


#Apply the prediction model to the testing data. The predicted classification are (and were 100% accurate):

```{r}
predTesting <- predict(fit, testing)

predTesting
```


# generate the files for submission with the given R code from the assignment.


```{r}

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(predTesting)
```